# SEA-LION v3.5

SEA-LION version 3.5, released in Apr 2025, is our first set of hybrid reasoning models trained on Southeast Asian data, each with their unique strengths:
- [Llama-SEA-LION-v3.5-8B-R](./llama-sea-lion-v3.5-8B.md), features a large context length of 128k tokens, enabling the model to handle extensive and complex dialogues effectively.
- [Llama-SEA-LION-v3.5-70B-R](./llama-sea-lion-v3.5-70B.md), is one of our largest model to date. The model also features a 128K context length, offering superior performance metrics compared to its predecessors and contemporaries.

Trained off our SEA-LION v3 models, SEA-LION v3.5 is explicitly enhanced for reasoning tasks with the inclusion of thinking blocks, continuing our mission to create language models that understand and respond with greater cultural awareness and depth across Southeast Asia. Mode selection is managed through the tokenizerâ€™s chat template and offers versatile functionality, handling both complex reasoning tasks and general text generation. 

For detailed information of each of the SEA-LION v3.5 models, please refer to their individual documentation pages via the links above.