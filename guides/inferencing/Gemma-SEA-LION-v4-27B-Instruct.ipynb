{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to deploy the SEA-LION v4 instruct (Gemma-SEA-LION-v4-27B-IT) for inference using Amazon SageMakerAI with LMI v15 powered by vLLM 0.8.4\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Gemma SEA-LION v4 27 B instruct model (HuggingFace model ID: [aisingapore/Gemma-SEA-LION-v4-27B-IT](https://huggingface.co/aisingapore/Gemma-SEA-LION-v4-27B-IT)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15 powered by vLLM 0.8.4](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "The SEA-LION v4 (Gemma3-based) models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. The base Gemma 3 model has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/google/gemma-3-27b-it) for license.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "2.251.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_session  = sagemaker.Session()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d3035e-f732-4429-a7a5-89bf8f822750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_name = gemma-sea-lion-v4-27b-it\n"
     ]
    }
   ],
   "source": [
    "HF_MODEL_ID = \"aisingapore/Gemma-SEA-LION-v4-27B-IT\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "print(f'base_name = {base_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  -  Using gemma-sea-lion-v4-27b-it \n",
    "  - Maximum sequence length of 32768 tokens\n",
    "  - model loading timeout of 1200 seconds (20 minutes)\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - Max rolling batch size of 16 for efficient batching\n",
    "  \n",
    "#### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory.\n",
    "\n",
    "1. Start with a conservative value (current: 32768)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ae607-c0a6-46f7-9ca1-df71893f6ed2",
   "metadata": {},
   "source": [
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb092ff-a52e-442f-890b-c7c6a7e3d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "option.entryPoint=djl_python.lmi_vllm.vllm_async_service\n",
    "option.tensor_parallel_degree=max\n",
    "option.model_loading_timeout=1200\n",
    "fail_fast=true\n",
    "option.max_model_len=32768\n",
    "option.max_rolling_batch_size=16\n",
    "option.trust_remote_code=true\n",
    "option.model_id={HF_MODEL_ID}\n",
    "option.revision=main\n",
    "\"\"\"\n",
    "\n",
    "# If you have copied the model data from HuggingFace to an S3 bucket, you can replace\n",
    "# option.model_id={HF_MODEL_ID}\n",
    "# with\n",
    "# option.model_id={model_s3_uri}\n",
    "\n",
    "with open(\"config/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb14d8df-060c-43ab-bd1e-45aa2d4369c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mengine\u001b[39;49;00m=\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.async_mode\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.rolling_batch\u001b[39;49;00m=\u001b[33mdisable\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.entryPoint\u001b[39;49;00m=\u001b[33mdjl_python.lmi_vllm.vllm_async_service\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33mmax\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.model_loading_timeout\u001b[39;49;00m=\u001b[33m1200\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mfail_fast\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.max_model_len\u001b[39;49;00m=\u001b[33m32768\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.max_rolling_batch_size\u001b[39;49;00m=\u001b[33m16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.trust_remote_code\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33maisingapore/Gemma-SEA-LION-v4-27B-IT\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.revision\u001b[39;49;00m=\u001b[33mmain\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# Check that the file config/serving.properties was generated properly\n",
    "!pygmentize config/serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ec7a-5409-4ba8-8b23-b99c4d4a3bae",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains `serving.properties` to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "config_files_uri = S3Uploader.upload(\n",
    "    local_path=\"config\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/config-files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e588bc54-3ee6-452c-ac69-f4dc28eb3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"code_model_uri: {config_files_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Gemma-3-27B-it, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G6e Instance](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6e.48xlarge` instance which offer:\n",
    "  - 8 NVIDIA L40S Tensor Core GPUs\n",
    "  - 384 GB of total GPU memory (48 GB of memory per GPU)\n",
    "  - up to 400 Gbps of network bandwidth\n",
    "  - up to 1.536 TB of system memory\n",
    "  - and up to 7.6 TB of local NVMe SSD storage.\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16a3759-4cce-4a69-9f77-68251aabbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6e.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\n"
     ]
    }
   ],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN \n",
    "Gemma-3-27B-Instruct is a gated model. Therefore, if you deploy model files hosted on the Hub, you need to provide your HuggingFace token as environment variable. This enables SageMaker AI to download the files at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b4ca9e-142d-41cb-82a8-15820c7232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed config files\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{config_files_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea1bb53-f5f3-490e-aca9-a9790481798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = \"hf_ (fill in you Hugging Face Hub token here)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8e520b-0793-4414-a3ad-f56a60547c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-sea-lion-v4-27b-it-250825-0926'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "527937f2-43e9-428a-b201-ce299894390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "sealion_4_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=2\", # Limit the number of images that can be sent per prompt\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HUGGING_FACE_HUB_TOKEN # HF Token for gated models\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6e instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6e.48xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f59d38-7987-496f-bb6a-671ed46580bf",
   "metadata": {},
   "source": [
    "The following may take approximately 11-12 minutes (or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb2f42-4790-4c12-850e-b7482c05be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "try:\n",
    "    sealion_4_model.deploy(\n",
    "        endpoint_name=endpoint_name,\n",
    "        initial_instance_count=1,\n",
    "        instance_type=gpu_instance_type\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83950a-ff8d-4344-add3-6c88b48b8d36",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcbe864c-6abc-4c6b-8595-a31a8ad5f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f10a937-2084-4071-a96c-12bce95709b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: gemma-sea-lion-v4-27b-it-250825-0928\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Automatically retrieve the first endpoint (if this is your only endpoint)\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "response = sagemaker_client.list_endpoints()\n",
    "endpoint_names = [ endpoint['EndpointName'] for endpoint in response['Endpoints'] ]\n",
    "if len(endpoint_names):\n",
    "    endpoint_name = endpoint_names[0]\n",
    "    print(f'Using endpoint: {endpoint_name}')\n",
    "\n",
    "# Option 2: Set the endpoint name manually (Uncomment below to use Option 2)\n",
    "# endpoint_name = \"gemma-3-27b-it-... (replace with your enpoint name)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff66831e-dc02-487f-a253-5caa915a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc088b5-0681-411b-9e1d-650736b34723",
   "metadata": {},
   "source": [
    "## Text only Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "720dff91-c842-48da-9f6b-aa9f4e276a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sealion(prompt: str, print_response=True, **kwargs):\n",
    "    payload = {\n",
    "        \"messages\" : [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    for k in kwargs:\n",
    "        if k in ['max_tokens', 'temperature', 'top_p']:\n",
    "            payload[k] = kwargs[k]\n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    if print_response:\n",
    "        # Print usage statistics\n",
    "        usage = response['usage']\n",
    "        print(response['choices'][0]['message']['content'].strip())\n",
    "        print(f\"=== Token Usage: {usage['prompt_tokens']} (prompt), {usage['completion_tokens']} (completions), {usage['total_tokens']} (total) ===\")\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0da6a906-d894-4916-b5b0-87ed91fe8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Algorithm's Dream\n",
      "\n",
      "No longer coded, line by rigid line,\n",
      "A new intelligence begins to shine.\n",
      "Machine Learning, a whisper, then a roar,\n",
      "Learning from data, wanting more and more.\n",
      "\n",
      "It starts with patterns, hidden in the haze,\n",
      "A million examples in a digital maze.\n",
      "Features extracted, a careful, keen eye,\n",
      "To find the connections as the moments fly.\n",
      "\n",
      "Regression's curve, predicting what will be,\n",
      "Classification sorting, for you and for me.\n",
      "Clustering groups, where similarities reside,\n",
      "Uncovering secrets, deep inside.\n",
      "\n",
      "Neural networks layered, a mimicking brain,\n",
      "Connections strengthening, again and again.\n",
      "Backpropagation's dance, a subtle, slow art,\n",
      "Adjusting the weights, playing a crucial part.\n",
      "\n",
      "From spam detection to faces it knows,\n",
      "From medical diagnoses to where the river flows.\n",
      "It learns to translate, to write, and to see,\n",
      "A powerful tool, for you and for me.\n",
      "\n",
      "But caution is needed, a mindful embrace,\n",
      "Bias can creep in, leaving a flawed trace.\n",
      "Explainability sought, a transparent view,\n",
      "To understand *why* it does what it do.\n",
      "\n",
      "So let the algorithms learn and evolve,\n",
      "A future unfolding, problems to resolve.\n",
      "The algorithm's dream, a world understood,\n",
      "Powered by data, for the common good.\n",
      "=== Token Usage: 17 (prompt), 290 (completions), 307 (total) ===\n"
     ]
    }
   ],
   "source": [
    "_ = invoke_sealion(\"Write me a poem about Machine Learning.\", max_tokens=500, temperature=0.1, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a90166e-38d3-4c05-9abb-149cc74f3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PROMPTS = [\n",
    "    \"\"\"Terjemahkan teks berikut ini ke dalam Bahasa Inggris. Teks: Anak laki-laki ini, yang secara teknis tidak diijinkan untuk memiliki akun situs ini untuk tiga tahun mendatang,menemukan sebuah bug (kesalahan akibat ketidaksempurnaan desain) yang memungkinkan dia menghapus komentar yang dibuat oleh pengguna lain. Masalah ini dengan “cepat” diperbaiki setelah ditemukan, demikian keterangan Facebook, perusahaan media sosial yang memiliki Instagram. Jani kemudian dibayar - yang membuat dia sebagai anak yang termuda yang pernah menerima hadiah atas penemuan bug ini. Setelah menemukan kekurangan itu pada Februari, dia mengirim email ke Facebook. Beli sepeda dan peralatan sepak bola Sejumlah ahli teknik keamanan di perusahaan itu telah membuat akun uji coba kepada Jani untuk membuktikan teorinya - dan dia dapat melakukannya. Anak laki-laki ini, dari Helsinki, mengatakan kepada koran Finlandia Iltalehti, dia berencana untuk menggunakan uang itu untuk membeli sepeda baru, peralatan sepak bola dan komputer untuk saudara laki-lakinya. Facebook mengatakan kepada BBC, telah membayar $4.3 juta sebagai hadiah bagi yang menemukan bug sejak 2011. Banyak perusahaan menawarkan sebuah insentif keuangan bagi profesional keamanan - dan anak-anak muda, yang menyampaikan kekurangan itu kepada perusahaan, dibandingkan menjualnya ke pasar gelap. Terjemahan:\"\"\",\n",
    "    \"\"\"Apa sentimen dari kalimat berikut ini? Kalimat: Buku ini sangat membosankan. Jawaban:\"\"\",\n",
    "    \"\"\"Anda akan diberikan sebuah teks dan pertanyaan. Jawablah pertanyaan tersebut berdasarkan teks yang tersedia. > Teks: “Isyana lahir di Bandung pada 2 Mei 1993. Dia menghabiskan masa kecilnya di berbagai lokasi, karena orang tuanya bekerja & melanjutkan studi mereka di Belgia. Namun, pada usia 7 tahun keluarganya pindah ke Bandung, Indonesia. Isyana adalah putri bungsu dari pasangan Luana Marpanda, seorang guru musik, dan Sapta Dwikardana, Ph.D seorang dosen dan terapis (grafologis). Ia memiliki kakak perempuan bernama Rara Sekar Larasati, yang juga merupakan vokalis band bernama Banda Neira. Dibesarkan dalam keluarga pendidik, Isyana diperkenalkan ke dunia musik pada usia 4 tahun oleh ibunya. Isyana telah menguasai sejumlah instrumen. Termasuk piano, electone, flute, biola, dan saksofon.” > Pertanyaan: Siapa nama orang tua Isyana?\"\"\",\n",
    "    \"\"\"Sebutkan persamaan dan perbedaan antara gado-gado, ketoprak dan karedok\"\"\",\n",
    "    \"\"\"Jelaskan budaya Indonesia menyapa orang yang lebih tua?\"\"\",\n",
    "    \"\"\"Jelaskan budaya pulang kampung ketika lebaran?\"\"\",\n",
    "    \"\"\"Sebutkan berbagai jenis kopi dan karakteristik rasanya yang berasal dari Indonesia\"\"\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dd6222e-b05e-4109-bf43-26ada733a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Prompt: Terjemahkan teks berikut ini ke dalam Bahasa Inggris. Teks: Anak laki-laki ini, yang secara teknis tidak diijinkan untuk memiliki akun situs ini untuk tiga tahun mendatang,menemukan sebuah bug (kesalahan akibat ketidaksempurnaan desain) yang memungkinkan dia menghapus komentar yang dibuat oleh pengguna lain. Masalah ini dengan “cepat” diperbaiki setelah ditemukan, demikian keterangan Facebook, perusahaan media sosial yang memiliki Instagram. Jani kemudian dibayar - yang membuat dia sebagai anak yang termuda yang pernah menerima hadiah atas penemuan bug ini. Setelah menemukan kekurangan itu pada Februari, dia mengirim email ke Facebook. Beli sepeda dan peralatan sepak bola Sejumlah ahli teknik keamanan di perusahaan itu telah membuat akun uji coba kepada Jani untuk membuktikan teorinya - dan dia dapat melakukannya. Anak laki-laki ini, dari Helsinki, mengatakan kepada koran Finlandia Iltalehti, dia berencana untuk menggunakan uang itu untuk membeli sepeda baru, peralatan sepak bola dan komputer untuk saudara laki-lakinya. Facebook mengatakan kepada BBC, telah membayar $4.3 juta sebagai hadiah bagi yang menemukan bug sejak 2011. Banyak perusahaan menawarkan sebuah insentif keuangan bagi profesional keamanan - dan anak-anak muda, yang menyampaikan kekurangan itu kepada perusahaan, dibandingkan menjualnya ke pasar gelap. Terjemahan: #####\n",
      "##### Response #####\n",
      "Here's the English translation of the provided text:\n",
      "\n",
      "This boy, who is technically not allowed to have an account on this site for the next three years, discovered a bug (an error caused by design imperfections) that allowed him to delete comments made by other users. The issue was “quickly” fixed after it was discovered, according to Facebook, the social media company that owns Instagram. Jani was then paid – making him the youngest person ever to receive a bounty for discovering this bug. After finding the vulnerability in February, he emailed Facebook. \n",
      "\n",
      "**Buy a bike and football equipment**\n",
      "\n",
      "A number of security engineering experts at the company created a test account for Jani to prove his theory – and he was able to do so. The boy, from Helsinki, told the Finnish newspaper Iltalehti he plans to use the money to buy a new bike, football equipment, and a computer for his brother. Facebook told the BBC it has paid out $4.3 million in bug bounty rewards since 2011. Many companies offer financial incentives to security professionals – and young people – who report vulnerabilities to them, rather than selling them on the dark market.\n",
      "=== Token Usage: 273 (prompt), 241 (completions), 514 (total) ===\n",
      "\n",
      "##### Prompt: Apa sentimen dari kalimat berikut ini? Kalimat: Buku ini sangat membosankan. Jawaban: #####\n",
      "##### Response #####\n",
      "Jawaban: **Negatif**\n",
      "\n",
      "Kalimat \"Buku ini sangat membosankan\" mengekspresikan ketidakpuasan dan perasaan tidak tertarik terhadap buku tersebut. Kata \"membosankan\" memiliki konotasi negatif yang jelas.\n",
      "=== Token Usage: 30 (prompt), 52 (completions), 82 (total) ===\n",
      "\n",
      "##### Prompt: Anda akan diberikan sebuah teks dan pertanyaan. Jawablah pertanyaan tersebut berdasarkan teks yang tersedia. > Teks: “Isyana lahir di Bandung pada 2 Mei 1993. Dia menghabiskan masa kecilnya di berbagai lokasi, karena orang tuanya bekerja & melanjutkan studi mereka di Belgia. Namun, pada usia 7 tahun keluarganya pindah ke Bandung, Indonesia. Isyana adalah putri bungsu dari pasangan Luana Marpanda, seorang guru musik, dan Sapta Dwikardana, Ph.D seorang dosen dan terapis (grafologis). Ia memiliki kakak perempuan bernama Rara Sekar Larasati, yang juga merupakan vokalis band bernama Banda Neira. Dibesarkan dalam keluarga pendidik, Isyana diperkenalkan ke dunia musik pada usia 4 tahun oleh ibunya. Isyana telah menguasai sejumlah instrumen. Termasuk piano, electone, flute, biola, dan saksofon.” > Pertanyaan: Siapa nama orang tua Isyana? #####\n",
      "##### Response #####\n",
      "Berdasarkan teks, nama orang tua Isyana adalah:\n",
      "\n",
      "*   **Luana Marpanda** (ibu)\n",
      "*   **Sapta Dwikardana, Ph.D** (ayah)\n",
      "=== Token Usage: 212 (prompt), 43 (completions), 255 (total) ===\n",
      "\n",
      "##### Prompt: Sebutkan persamaan dan perbedaan antara gado-gado, ketoprak dan karedok #####\n",
      "##### Response #####\n",
      "Tentu, mari kita bahas persamaan dan perbedaan antara gado-gado, ketoprak, dan karedok:\n",
      "\n",
      "**Persamaan:**\n",
      "\n",
      "*   **Bahan Dasar:** Ketiga hidangan ini sama-sama menggunakan sayuran rebus sebagai bahan utama. Sayuran yang umum digunakan meliputi kacang panjang, bayam, tauge, kentang, tahu, tempe, dan telur rebus.\n",
      "*   **Saus Kacang:** Ketiganya disajikan dengan saus kacang sebagai bumbu utama. Meskipun resep saus kacang bisa sedikit berbeda, bahan dasarnya tetap kacang tanah goreng yang dihaluskan.\n",
      "*   **Asal Daerah:** Ketiga hidangan ini berasal dari Jawa Barat, Indonesia.\n",
      "*   **Makanan Sehat:** Ketiga hidangan ini relatif sehat karena kaya akan serat, protein, dan vitamin dari sayuran.\n",
      "\n",
      "**Perbedaan:**\n",
      "\n",
      "| Fitur          | Gado-Gado                               | Ketoprak                               | Karedok                               |\n",
      "| -------------- | --------------------------------------- | --------------------------------------- | --------------------------------------- |\n",
      "| **Bahan Tambahan** | Lontong/ketupat, kerupuk, emping melinjo | Lontong/ketupat, kerupuk, bawang goreng | Tidak menggunakan lontong/ketupat/kerupuk |\n",
      "| **Saus Kacang**  | Lebih manis dan kental, sering ditambahkan sedikit asam jawa | Lebih gurih dan sedikit pedas, menggunakan bumbu kacang yang lebih kuat | Lebih segar dan ringan, menggunakan sedikit terasi dan cabai rawit |\n",
      "| **Penyajian**    | Sayuran dan bahan lain dicampur dengan saus kacang | Sayuran dan bahan lain disiram dengan saus kacang | Sayuran disajikan mentah atau setengah matang dengan saus kacang di sampingnya (dipisah) |\n",
      "| **Tekstur**      | Lebih lembut karena sayuran direbus lebih lama | Lebih renyah karena ada kerupuk dan bawang goreng | Lebih segar dan renyah karena sayuran mentah |\n",
      "| **Ciri Khas**    | Rasa manis yang dominan, sering disajikan dengan kerupuk udang | Rasa gurih dan pedas yang kuat, sering disajikan dengan kerupuk kulit | Rasa segar dan alami dari sayuran, menggunakan sambal karedok yang khas |\n",
      "\n",
      "**Sing\n",
      "=== Token Usage: 28 (prompt), 500 (completions), 528 (total) ===\n",
      "\n",
      "##### Prompt: Jelaskan budaya Indonesia menyapa orang yang lebih tua? #####\n",
      "##### Response #####\n",
      "Budaya Indonesia sangat menjunjung tinggi rasa hormat kepada orang yang lebih tua. Cara menyapa orang yang lebih tua di Indonesia sangat beragam, tergantung pada daerah, etnis, dan tingkat keakraban. Namun, ada beberapa prinsip umum dan contoh yang bisa dijelaskan:\n",
      "\n",
      "**Prinsip Umum:**\n",
      "\n",
      "*   **Menunduk:** Menundukkan kepala sedikit saat menyapa adalah tanda hormat yang umum di seluruh Indonesia. Ini menunjukkan kerendahan hati dan pengakuan atas status yang lebih tinggi.\n",
      "*   **Menggunakan Sapaan Hormat:** Bahasa Indonesia memiliki banyak sapaan yang menunjukkan rasa hormat, seperti:\n",
      "    *   **Bapak/Ibu:** Sapaan paling umum untuk orang yang lebih tua, terlepas dari status pernikahan.\n",
      "    *   **Kakek/Nenek:** Digunakan untuk kakek dan nenek sendiri, atau orang yang lebih tua yang dianggap seperti kakek/nenek.\n",
      "    *   **Pakde/Bude:** Digunakan di Jawa untuk paman dan bibi dari pihak ayah.\n",
      "    *   **Kakang/Mbak:** Digunakan di Jawa untuk kakak laki-laki/perempuan, tetapi juga bisa digunakan untuk orang yang lebih tua yang dianggap seperti kakak.\n",
      "    *   **Inang:** Digunakan di Sumatera Utara untuk ibu atau wanita yang lebih tua.\n",
      "    *   **Daeng:** Digunakan di Sulawesi Selatan untuk orang yang dihormati.\n",
      "*   **Menggunakan Bahasa yang Sopan:** Hindari bahasa gaul atau bahasa informal saat berbicara dengan orang yang lebih tua. Gunakan bahasa Indonesia yang baik dan benar, atau bahasa daerah yang sopan.\n",
      "*   **Menawarkan Bantuan:** Menawarkan bantuan kepada orang yang lebih tua, seperti membawakan barang atau membantu mereka berjalan, adalah tindakan yang sangat dihargai.\n",
      "*   **Menunggu Mereka Berbicara Dahulu:** Dalam percakapan, biarkan orang yang lebih tua berbicara terlebih dahulu. Dengarkan dengan penuh perhatian dan jangan menyela.\n",
      "*   **Menghindari Kontak Mata Langsung yang Terlalu Lama:** Meskipun kontak mata penting dalam komunikasi, menatap terlalu lama bisa dianggap tidak sopan, terutama kepada orang yang lebih tua.\n",
      "\n",
      "**Contoh Cara Menyapa Berdasarkan Daerah:**\n",
      "\n",
      "*   **Jawa:**\n",
      "    *   **\"Sugeng ndalu, Bapak/Ibu\"** (Selamat sore, Bapak/Ibu\n",
      "=== Token Usage: 21 (prompt), 500 (completions), 521 (total) ===\n",
      "\n",
      "##### Prompt: Jelaskan budaya pulang kampung ketika lebaran? #####\n",
      "##### Response #####\n",
      "Budaya pulang kampung saat Lebaran, atau yang dikenal dengan istilah \"mudik\" di Indonesia, adalah fenomena sosial dan budaya yang sangat kuat dan khas. Berikut penjelasan lengkapnya:\n",
      "\n",
      "**1. Definisi dan Latar Belakang:**\n",
      "\n",
      "*   **Mudik:** Secara harfiah berarti \"kembali\". Dalam konteks Lebaran, mudik adalah perjalanan pulang ke kampung halaman, biasanya ke desa atau kota asal keluarga, yang dilakukan oleh perantau atau orang yang bekerja dan tinggal di kota besar.\n",
      "*   **Latar Belakang:** Tradisi ini berakar kuat pada nilai-nilai kekeluargaan, gotong royong, dan tradisi Islam. Lebaran adalah momen penting untuk mempererat tali silaturahmi dengan keluarga besar, meminta maaf atas kesalahan, dan berbagi kebahagiaan. Dahulu, mudik dilakukan karena keterbatasan komunikasi dan transportasi, sehingga Lebaran menjadi satu-satunya waktu untuk berkumpul dengan keluarga. Meskipun teknologi modern telah berkembang, tradisi ini tetap bertahan dan bahkan semakin kuat.\n",
      "\n",
      "**2. Prosesi dan Tradisi Mudik:**\n",
      "\n",
      "*   **Persiapan:** Persiapan mudik biasanya dimulai jauh-jauh hari. Ini meliputi:\n",
      "    *   **Pembelian Tiket:** Membeli tiket transportasi (kereta api, bus, pesawat, kapal laut) yang seringkali sangat sulit karena tingginya permintaan.\n",
      "    *   **Persiapan Barang Bawaan:** Mempersiapkan pakaian lebaran, oleh-oleh untuk keluarga, dan kebutuhan lainnya.\n",
      "    *   **Membersihkan Rumah:** Membersihkan dan merapikan rumah di kampung halaman, seringkali dibantu oleh tetangga atau keluarga yang sudah lebih dulu tiba.\n",
      "    *   **Zakat Fitrah:** Membayar zakat fitrah sebagai kewajiban agama sebelum Lebaran.\n",
      "*   **Perjalanan:** Perjalanan mudik seringkali menjadi tantangan tersendiri karena:\n",
      "    *   **Kemacetan:** Kemacetan parah di jalan raya, terutama di jalur-jalur utama menuju kampung halaman.\n",
      "    *   **Kepadatan Penumpang:** Kepadatan penumpang di berbagai moda transportasi.\n",
      "    *   **Kelelahan:** Perjalanan yang panjang dan melelahkan.\n",
      "*   **Tiba di Kampung Halaman:** Kedatangan di kampung halaman disambut dengan hangat oleh keluarga dan tetangga.\n",
      "*   **Tradisi Lebaran:** Setelah tiba, berbagai tradisi\n",
      "=== Token Usage: 19 (prompt), 500 (completions), 519 (total) ===\n",
      "\n",
      "##### Prompt: Sebutkan berbagai jenis kopi dan karakteristik rasanya yang berasal dari Indonesia #####\n",
      "##### Response #####\n",
      "Indonesia adalah surga bagi pecinta kopi, dengan berbagai jenis kopi yang memiliki karakteristik rasa unik. Berikut adalah beberapa jenis kopi Indonesia yang populer beserta karakteristik rasanya:\n",
      "\n",
      "**1. Kopi Gayo (Aceh)**\n",
      "\n",
      "*   **Asal:** Dataran Tinggi Gayo, Aceh Tengah dan Bener Meriah.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Penuh dan tebal.\n",
      "    *   **Aroma:** Floral, herbal, dan sedikit rempah.\n",
      "    *   **Rasa:** Kompleks, dengan sentuhan cokelat, karamel, dan sedikit rasa buah (seperti buah beri atau apel).  Seringkali memiliki rasa *earthy* (tanah) yang khas.\n",
      "    *   **Acidity:** Sedang hingga tinggi, memberikan kesegaran.\n",
      "*   **Grade:** Umumnya Grade 1 (kualitas terbaik).\n",
      "\n",
      "**2. Kopi Toraja (Sulawesi Selatan)**\n",
      "\n",
      "*   **Asal:** Dataran Tinggi Toraja, Sulawesi Selatan.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Penuh dan lembut.\n",
      "    *   **Aroma:** Floral, fruity, dan sedikit *spicy*.\n",
      "    *   **Rasa:** Cokelat gelap, kacang-kacangan, dan sedikit rasa buah (seperti jeruk atau anggur).  Toraja dikenal dengan rasa *fruity* yang unik.\n",
      "    *   **Acidity:** Rendah hingga sedang, memberikan rasa yang lembut di lidah.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**3. Kopi Mandailing (Sumatera Utara)**\n",
      "\n",
      "*   **Asal:** Daerah Mandailing, Sumatera Utara.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sangat penuh dan tebal.\n",
      "    *   **Aroma:** Earthy, herbal, dan sedikit rempah.\n",
      "    *   **Rasa:** Cokelat gelap, rempah-rempah, dan sedikit rasa tanah.  Mandailing seringkali memiliki rasa *earthy* yang kuat.\n",
      "    *   **Acidity:** Rendah, memberikan rasa yang lembut dan tidak asam.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**4. Kopi Lintong (Sumatera Utara)**\n",
      "\n",
      "*   **Asal:** Daerah Lintong, Sumatera Utara.\n",
      "=== Token Usage: 23 (prompt), 500 (completions), 523 (total) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in SAMPLE_PROMPTS:\n",
    "    print(f\"##### Prompt: {prompt} #####\")\n",
    "    print(f\"##### Response #####\")\n",
    "    _ = invoke_sealion(prompt, max_tokens=500, temperature=0.1, top_p=0.9)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "## Multimodality\n",
    "\n",
    "SEA LION v4 models (based on Gemma 3) are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731037a-2649-4b40-b530-135d36ae706c",
   "metadata": {},
   "source": [
    "#### single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36b8a8a0-fde2-4286-b1cf-30bf6b069800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(url=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23894c86-9b8e-463c-84d1-dd6eef6f8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sealion_multimodal(prompt: str, image_url: str, system_prompt=\"You are a helpful assistant.\", print_response=True, **kwargs):\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": system_prompt\n",
    "                  }\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\", \n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    for k in kwargs:\n",
    "        if k in ['max_tokens', 'temperature', 'top_p']:\n",
    "            payload[k] = kwargs[k]\n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    if print_response:\n",
    "        # Print usage statistics\n",
    "        usage = response['usage']\n",
    "        print(response['choices'][0]['message']['content'].strip())\n",
    "        print(f\"=== Token Usage: {usage['prompt_tokens']} (prompt), {usage['completion_tokens']} (completions), {usage['total_tokens']} (total) ===\")\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74679f16-f21f-4924-b2bf-ebe2ec6aab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, the animal on the candy appears to be a **turtle**. Each candy has a small turtle design on it.\n",
      "=== Token Usage: 282 (prompt), 28 (completions), 310 (total) ===\n"
     ]
    }
   ],
   "source": [
    "_ = invoke_sealion_multimodal(\n",
    "    prompt = \"What animal is on the candy?\",\n",
    "    image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f842f1e-1f59-496b-85ee-3b0a3bd85951",
   "metadata": {},
   "source": [
    "### Streaming responses\n",
    "You can also direclty stream response from your endpoint. To achieve this, we will use the invoke_endpoint_with_response_stream API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362629-77fe-4b57-b1ab-97259defc72b",
   "metadata": {},
   "source": [
    "You can **interleave images with text**. To do so, just cut off the input text where you want to insert an image, and insert it with an image block like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54a9c62e-fb42-409a-b1f5-0c4084662fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "# IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3018.JPG\", height=300, width= 300)\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf3fb62f-16c3-4eef-9f6e-9f82ba78cc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "# IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\", height=300, width= 300)\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b8d4f5f-fc32-468f-b4d0-3551c9327ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"I have this \"},  # \"I'm already using this supplement \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"and I want to use this as well \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\"}, # \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \" what are cautions?\"},\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 1500,\n",
    "  \"temperature\": 0.6,\n",
    "  \"top_p\": 0.9,\n",
    "  \"stream\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27e3b708-cf53-402d-8dff-d3b465775fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: gemma-sea-lion-v4-27b-it-250825-0928\n",
      "Response: , see you a picture of what appears to be jelly beans (or similar candies) in a hand, and a picture of fruit and a knife on a cutting board. \n",
      "\n",
      "Here are some cautions combining these images, assuming you're thinking about combining them in some way (like a composite image, a story, or a concept):\n",
      "\n",
      "**1. Health & Safety (Especially if portraying consumption):**\n",
      "\n",
      "*   **Sugar Content:** The candies are high in sugar. Combining this with fruit could reinforce a message of excessive sugar intake. If you're creating content for children, be mindful of this.\n",
      "*   **Knife Safety:** The knife in the second image is a potential hazard. If you're depicting someone using it, emphasize safe handling. Avoid any imagery that could encourage dangerous behavior.\n",
      "*   **Choking Hazard:** Small candies can be a choking hazard, especially for young children. If your image includes children, avoid showing them putting the candies in their mouths.\n",
      "\n",
      "**2. Conceptual/Message Considerations:**\n",
      "\n",
      "*   **Contrasting Themes:** The images present a contrast between processed sweets and natural fruit. Consider what message you want to convey with this juxtaposition. Is it about healthy choices, indulgence, or something else?\n",
      "*   **Visual Balance:** When combining the images, think about visual balance. The colors and textures are quite different.\n",
      "*   **Context is Key:** The meaning of the combination depends heavily on the context. A simple image of fruit and candy might be harmless, but if it's used in an advertisement or to promote a specific diet, it could be misleading.\n",
      "\n",
      "**3. Potential Misinterpretation:**\n",
      "\n",
      "*   **Food Safety:** Be careful not to create an image that implies the candies are *made from* or *mixed with* the fruit in an unsafe way.\n",
      "\n",
      "\n",
      "\n",
      "**To help me give you more specific cautions, could you tell me:**\n",
      "\n",
      "*   **What are you planning to *do* with these images?** (e.g., create a meme, a digital artwork, a story, a presentation, etc.)\n",
      "*   **Who is your target audience?** (e.g., children, adults, general public)\n",
      "*   **What message are you trying to convey?**\n",
      "\n",
      "\n",
      "\n",
      "**Disclaimer:** *I am an AI and cannot provide professional health or safety advice. These are general cautions based on the images provided.*\n",
      "\n",
      "Metrics:\n",
      "Time to First Token (TTFT): 0.01 seconds\n",
      "Total Tokens Generated: 491\n",
      "Total Latency: 8.84 seconds\n",
      "Tokens per second: 55.57\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "print(f'Using endpoint: {endpoint_name}')\n",
    "\n",
    "# Invoke the model\n",
    "response_stream = sagemaker_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = \"application/json\",\n",
    "    Body = json.dumps(body)\n",
    ")\n",
    "\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_count += 1\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                                    # Record time to first token\n",
    "                    if not first_token_received:\n",
    "                        ttft = time.time() - start_time\n",
    "                        first_token_received = True\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "# Print metrics after completion\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds\" if ttft else \"TTFT: N/A\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "if token_count > 0 and total_latency > 0:\n",
    "    print(f\"Tokens per second: {token_count/total_latency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9783d3-a8e4-4c86-81f8-054e2175ce5a",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e2269-2f95-49e3-803f-5110d2023b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to generate html version of the Jupyter notebook\n",
    "!jupyter nbconvert Gemma-SEA-LION-v4-27B-Instruct.ipynb --to html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
