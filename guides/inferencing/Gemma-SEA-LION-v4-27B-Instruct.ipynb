{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to deploy the SEA-LION v4 instruct (Gemma-SEA-LION-v4-27B-IT) for inference using Amazon SageMakerAI with LMI v15 powered by vLLM 0.8.4\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Gemma SEA-LION v4 27 B instruct model (HuggingFace model ID: [aisingapore/Gemma-SEA-LION-v4-27B-IT](https://huggingface.co/aisingapore/Gemma-SEA-LION-v4-27B-IT)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15 powered by vLLM 0.8.4](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "The SEA-LION v4 (Gemma3-based) models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. The base Gemma 3 model has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/google/gemma-3-27b-it) for license.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "2.251.1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_session  = sagemaker.Session()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d3035e-f732-4429-a7a5-89bf8f822750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_name = gemma-sea-lion-v4-27b-it\n"
     ]
    }
   ],
   "source": [
    "HF_MODEL_ID = \"aisingapore/Gemma-SEA-LION-v4-27B-IT\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "print(f'base_name = {base_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  -  Using gemma-sea-lion-v4-27b-it \n",
    "  - Maximum sequence length of 32768 tokens\n",
    "  - model loading timeout of 1200 seconds (20 minutes)\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - Max rolling batch size of 16 for efficient batching\n",
    "  \n",
    "#### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory.\n",
    "\n",
    "1. Start with a conservative value (8192 has been tested successfully with g5.12xlarge, 32768 with g6e.48xlarge)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ae607-c0a6-46f7-9ca1-df71893f6ed2",
   "metadata": {},
   "source": [
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb092ff-a52e-442f-890b-c7c6a7e3d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "option.entryPoint=djl_python.lmi_vllm.vllm_async_service\n",
    "option.tensor_parallel_degree=max\n",
    "option.model_loading_timeout=1200\n",
    "fail_fast=true\n",
    "option.max_model_len=8192\n",
    "option.max_rolling_batch_size=16\n",
    "option.trust_remote_code=true\n",
    "option.model_id={HF_MODEL_ID}\n",
    "option.revision=main\n",
    "\"\"\"\n",
    "\n",
    "# If you have copied the model data from HuggingFace to an S3 bucket, you can replace\n",
    "# option.model_id={HF_MODEL_ID}\n",
    "# with\n",
    "# option.model_id={model_s3_uri}\n",
    "\n",
    "with open(\"config/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb14d8df-060c-43ab-bd1e-45aa2d4369c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mengine\u001b[39;49;00m=\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.async_mode\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.rolling_batch\u001b[39;49;00m=\u001b[33mdisable\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.entryPoint\u001b[39;49;00m=\u001b[33mdjl_python.lmi_vllm.vllm_async_service\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33mmax\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.model_loading_timeout\u001b[39;49;00m=\u001b[33m1200\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mfail_fast\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.max_model_len\u001b[39;49;00m=\u001b[33m8192\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.max_rolling_batch_size\u001b[39;49;00m=\u001b[33m16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.trust_remote_code\u001b[39;49;00m=\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33maisingapore/Gemma-SEA-LION-v4-27B-IT\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36moption.revision\u001b[39;49;00m=\u001b[33mmain\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# Check that the file config/serving.properties was generated properly\n",
    "!pygmentize config/serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ec7a-5409-4ba8-8b23-b99c4d4a3bae",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains `serving.properties` to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "config_files_uri = S3Uploader.upload(\n",
    "    local_path=\"config\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/config-files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e588bc54-3ee6-452c-ac69-f4dc28eb3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"code_model_uri: {config_files_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Gemma-SEA-LION-v4-27B-IT, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G5 Instances](https://aws.amazon.com/ec2/instance-types/g5/)**: AWS's GPU instance type powered by NVIDIA A10G Tensor Core GPUs, or **[G6e Instances](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g5.12xlarge` instance which offers:\n",
    "  - 4 NVIDIA A10G Tensor Core GPUs\n",
    "  - 96 GB of total GPU memory (24 GB of memory per GPU)\n",
    "  - up to 40 Gbps of network bandwidth (16 Gbps of EBS bandwidth)\n",
    "  - up to 192 GB of system memory\n",
    "  - and up to 3.8 TB of local NVMe SSD storage.\n",
    "- We can also use `ml.g6e.48xlarge` instance which offers:\n",
    "  - 8 NVIDIA L40S Tensor Core GPUs\n",
    "  - 384 GB of total GPU memory (48 GB of memory per GPU)\n",
    "  - up to 400 Gbps of network bandwidth (60 Gbps of EBS bandwidth)\n",
    "  - up to 1.536 TB of system memory\n",
    "  - and up to 7.6 TB of local NVMe SSD storage.\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d359347-fbc1-4aaf-b489-20ac63bfa519",
   "metadata": {},
   "source": [
    "Note the following costs of running ml.g5.12xlarge, and ml.g6e.48xlarge instances in us-east-1 (SageMaker Realtime Inference):\n",
    "\n",
    "Instance Type     | Cost/hr   | Cost/day  | Cost/month\n",
    "------------------|-----------|-----------|-------------\n",
    "`ml.g5.12xlarge`  | USD  7.09 |USD 170.16 |USD  5,175.70\n",
    "`ml.g6e.48xlarge` | USD 37.664|USD 903.936|USD 27,494.72\n",
    "\n",
    "If you are not using the instances 24/7, it is highly recommended that you consider deleting the endpoints, or choosing an option such as [SageMaker Scale down to zero feature](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-zero-instances.html) as mentioned in this blog post: [Unlock cost savings with the new scale down to zero feature in SageMaker Inference](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a3759-4cce-4a69-9f77-68251aabbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_instance_type = \"ml.g6e.48xlarge\"\n",
    "gpu_instance_type = \"ml.g5.12xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\n"
     ]
    }
   ],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN\n",
    "This is not required for Gemma-SEA-LION-v4-27B-IT. Note that it is required for Gemma-3-27B-Instruct, which is a gated model. If you deploy model files hosted on the Hub, you need to provide your HuggingFace token as environment variable. This enables SageMaker AI to download the files at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b4ca9e-142d-41cb-82a8-15820c7232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed config files\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{config_files_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea1bb53-f5f3-490e-aca9-a9790481798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = \"hf_ (fill in you Hugging Face Hub token here)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca8e520b-0793-4414-a3ad-f56a60547c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-sea-lion-v4-27b-it-250901-0658'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527937f2-43e9-428a-b201-ce299894390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# sealion_4_model = Model(\n",
    "#     name = model_name,\n",
    "#     image_uri=image_uri,\n",
    "#     model_data=model_data,  # Path to uncompressed code files\n",
    "#     role=role,\n",
    "#     env={\n",
    "#         \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "#         \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=2\", # Limit the number of images that can be sent per prompt\n",
    "#         \"HUGGING_FACE_HUB_TOKEN\": HUGGING_FACE_HUB_TOKEN # HF Token for gated models\n",
    "#     },\n",
    "# )\n",
    "sealion_4_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=2\" # Limit the number of images that can be sent per prompt\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6e instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g5.12xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f59d38-7987-496f-bb6a-671ed46580bf",
   "metadata": {},
   "source": [
    "The following may take approximately 11-12 minutes (or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafb2f42-4790-4c12-850e-b7482c05be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------!CPU times: user 440 ms, sys: 35.9 ms, total: 476 ms\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "try:\n",
    "    sealion_4_model.deploy(\n",
    "        endpoint_name=endpoint_name,\n",
    "        initial_instance_count=1,\n",
    "        instance_type=gpu_instance_type\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83950a-ff8d-4344-add3-6c88b48b8d36",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcbe864c-6abc-4c6b-8595-a31a8ad5f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f10a937-2084-4071-a96c-12bce95709b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: gemma-sea-lion-v4-27b-it-250901-0659\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Automatically retrieve the first endpoint (if this is your only endpoint)\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "response = sagemaker_client.list_endpoints()\n",
    "endpoint_names = [ endpoint['EndpointName'] for endpoint in response['Endpoints'] ]\n",
    "if len(endpoint_names):\n",
    "    endpoint_name = endpoint_names[0]\n",
    "    print(f'Using endpoint: {endpoint_name}')\n",
    "\n",
    "# Option 2: Set the endpoint name manually (Uncomment below to use Option 2)\n",
    "# endpoint_name = \"gemma-3-27b-it-... (replace with your enpoint name)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff66831e-dc02-487f-a253-5caa915a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc088b5-0681-411b-9e1d-650736b34723",
   "metadata": {},
   "source": [
    "## Text only Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720dff91-c842-48da-9f6b-aa9f4e276a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sealion(prompt: str, print_response=True, **kwargs):\n",
    "    payload = {\n",
    "        \"messages\" : [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    for k in kwargs:\n",
    "        if k in ['max_tokens', 'temperature', 'top_p']:\n",
    "            payload[k] = kwargs[k]\n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    if print_response:\n",
    "        # Print usage statistics\n",
    "        usage = response['usage']\n",
    "        print(response['choices'][0]['message']['content'].strip())\n",
    "        print(f\"=== Token Usage: {usage['prompt_tokens']} (prompt), {usage['completion_tokens']} (completions), {usage['total_tokens']} (total) ===\")\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da6a906-d894-4916-b5b0-87ed91fe8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Algorithm's Dream\n",
      "\n",
      "No longer coded, line by rigid line,\n",
      "A different path, a future intertwined.\n",
      "Machine Learning wakes, a nascent mind,\n",
      "To learn from data, of every kind.\n",
      "\n",
      "It sifts through oceans, a digital tide,\n",
      "Of patterns hidden, where truths reside.\n",
      "From labeled examples, a knowledge gleaned,\n",
      "A predictive power, subtly seen.\n",
      "\n",
      "Regression's curve, a gentle slope,\n",
      "Classification's choice, fueled by hope.\n",
      "Clustering's dance, to group and define,\n",
      "Uncovering structures, beautifully aligned.\n",
      "\n",
      "Neural networks bloom, a complex array,\n",
      "Layers connecting, day by day.\n",
      "Backpropagation's flow, a constant quest,\n",
      "To minimize error, and be the best.\n",
      "\n",
      "From spam detection to faces it knows,\n",
      "From medical diagnoses to where the river flows.\n",
      "It learns to translate, to write and to see,\n",
      "A growing intelligence, for you and for me.\n",
      "\n",
      "But caution whispers, a thoughtful plea,\n",
      "Bias can linger, for all to see.\n",
      "Transparency needed, a guiding light,\n",
      "To build with fairness, and do what is right.\n",
      "\n",
      "So let the algorithms learn and grow,\n",
      "But with wisdom and ethics, let the knowledge flow.\n",
      "For in this machine, a potential lies,\n",
      "To shape a future, before our eyes.\n",
      "=== Token Usage: 17 (prompt), 283 (completions), 300 (total) ===\n"
     ]
    }
   ],
   "source": [
    "_ = invoke_sealion(\"Write me a poem about Machine Learning.\", max_tokens=500, temperature=0.1, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a90166e-38d3-4c05-9abb-149cc74f3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PROMPTS = [\n",
    "    \"\"\"Terjemahkan teks berikut ini ke dalam Bahasa Inggris. Teks: Anak laki-laki ini, yang secara teknis tidak diijinkan untuk memiliki akun situs ini untuk tiga tahun mendatang,menemukan sebuah bug (kesalahan akibat ketidaksempurnaan desain) yang memungkinkan dia menghapus komentar yang dibuat oleh pengguna lain. Masalah ini dengan “cepat” diperbaiki setelah ditemukan, demikian keterangan Facebook, perusahaan media sosial yang memiliki Instagram. Jani kemudian dibayar - yang membuat dia sebagai anak yang termuda yang pernah menerima hadiah atas penemuan bug ini. Setelah menemukan kekurangan itu pada Februari, dia mengirim email ke Facebook. Beli sepeda dan peralatan sepak bola Sejumlah ahli teknik keamanan di perusahaan itu telah membuat akun uji coba kepada Jani untuk membuktikan teorinya - dan dia dapat melakukannya. Anak laki-laki ini, dari Helsinki, mengatakan kepada koran Finlandia Iltalehti, dia berencana untuk menggunakan uang itu untuk membeli sepeda baru, peralatan sepak bola dan komputer untuk saudara laki-lakinya. Facebook mengatakan kepada BBC, telah membayar $4.3 juta sebagai hadiah bagi yang menemukan bug sejak 2011. Banyak perusahaan menawarkan sebuah insentif keuangan bagi profesional keamanan - dan anak-anak muda, yang menyampaikan kekurangan itu kepada perusahaan, dibandingkan menjualnya ke pasar gelap. Terjemahan:\"\"\",\n",
    "    \"\"\"Apa sentimen dari kalimat berikut ini? Kalimat: Buku ini sangat membosankan. Jawaban:\"\"\",\n",
    "    \"\"\"Anda akan diberikan sebuah teks dan pertanyaan. Jawablah pertanyaan tersebut berdasarkan teks yang tersedia. > Teks: “Isyana lahir di Bandung pada 2 Mei 1993. Dia menghabiskan masa kecilnya di berbagai lokasi, karena orang tuanya bekerja & melanjutkan studi mereka di Belgia. Namun, pada usia 7 tahun keluarganya pindah ke Bandung, Indonesia. Isyana adalah putri bungsu dari pasangan Luana Marpanda, seorang guru musik, dan Sapta Dwikardana, Ph.D seorang dosen dan terapis (grafologis). Ia memiliki kakak perempuan bernama Rara Sekar Larasati, yang juga merupakan vokalis band bernama Banda Neira. Dibesarkan dalam keluarga pendidik, Isyana diperkenalkan ke dunia musik pada usia 4 tahun oleh ibunya. Isyana telah menguasai sejumlah instrumen. Termasuk piano, electone, flute, biola, dan saksofon.” > Pertanyaan: Siapa nama orang tua Isyana?\"\"\",\n",
    "    \"\"\"Sebutkan persamaan dan perbedaan antara gado-gado, ketoprak dan karedok\"\"\",\n",
    "    \"\"\"Jelaskan budaya Indonesia menyapa orang yang lebih tua?\"\"\",\n",
    "    \"\"\"Jelaskan budaya pulang kampung ketika lebaran?\"\"\",\n",
    "    \"\"\"Sebutkan berbagai jenis kopi dan karakteristik rasanya yang berasal dari Indonesia\"\"\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd6222e-b05e-4109-bf43-26ada733a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Prompt: Terjemahkan teks berikut ini ke dalam Bahasa Inggris. Teks: Anak laki-laki ini, yang secara teknis tidak diijinkan untuk memiliki akun situs ini untuk tiga tahun mendatang,menemukan sebuah bug (kesalahan akibat ketidaksempurnaan desain) yang memungkinkan dia menghapus komentar yang dibuat oleh pengguna lain. Masalah ini dengan “cepat” diperbaiki setelah ditemukan, demikian keterangan Facebook, perusahaan media sosial yang memiliki Instagram. Jani kemudian dibayar - yang membuat dia sebagai anak yang termuda yang pernah menerima hadiah atas penemuan bug ini. Setelah menemukan kekurangan itu pada Februari, dia mengirim email ke Facebook. Beli sepeda dan peralatan sepak bola Sejumlah ahli teknik keamanan di perusahaan itu telah membuat akun uji coba kepada Jani untuk membuktikan teorinya - dan dia dapat melakukannya. Anak laki-laki ini, dari Helsinki, mengatakan kepada koran Finlandia Iltalehti, dia berencana untuk menggunakan uang itu untuk membeli sepeda baru, peralatan sepak bola dan komputer untuk saudara laki-lakinya. Facebook mengatakan kepada BBC, telah membayar $4.3 juta sebagai hadiah bagi yang menemukan bug sejak 2011. Banyak perusahaan menawarkan sebuah insentif keuangan bagi profesional keamanan - dan anak-anak muda, yang menyampaikan kekurangan itu kepada perusahaan, dibandingkan menjualnya ke pasar gelap. Terjemahan: #####\n",
      "##### Response #####\n",
      "Here's the English translation of the provided text:\n",
      "\n",
      "This boy, who is technically not allowed to have an account on this site for the next three years, discovered a bug (an error caused by design imperfections) that allowed him to delete comments made by other users. Facebook, the social media company that owns Instagram, stated the issue was “quickly” fixed after it was discovered. Jani was then paid – making him the youngest person ever to receive a bounty for finding this bug. After discovering the vulnerability in February, he emailed Facebook. \n",
      "\n",
      "**Buy a bike and football equipment**\n",
      "\n",
      "A number of security engineering experts at the company created a test account for Jani to prove his theory – and he was able to do so. The boy, from Helsinki, told the Finnish newspaper Iltalehti he plans to use the money to buy a new bike, football equipment, and a computer for his brother. Facebook told the BBC it has paid out $4.3 million in bug bounty rewards since 2011. Many companies offer financial incentives to security professionals – and young people – who report vulnerabilities to them, rather than selling them on the dark market.\n",
      "=== Token Usage: 273 (prompt), 240 (completions), 513 (total) ===\n",
      "\n",
      "##### Prompt: Apa sentimen dari kalimat berikut ini? Kalimat: Buku ini sangat membosankan. Jawaban: #####\n",
      "##### Response #####\n",
      "Jawaban: **Negatif**\n",
      "\n",
      "Kalimat \"Buku ini sangat membosankan\" mengekspresikan ketidakpuasan dan perasaan tidak tertarik terhadap buku tersebut. Kata \"membosankan\" memiliki konotasi negatif, sehingga sentimen keseluruhan kalimat adalah negatif.\n",
      "=== Token Usage: 30 (prompt), 58 (completions), 88 (total) ===\n",
      "\n",
      "##### Prompt: Anda akan diberikan sebuah teks dan pertanyaan. Jawablah pertanyaan tersebut berdasarkan teks yang tersedia. > Teks: “Isyana lahir di Bandung pada 2 Mei 1993. Dia menghabiskan masa kecilnya di berbagai lokasi, karena orang tuanya bekerja & melanjutkan studi mereka di Belgia. Namun, pada usia 7 tahun keluarganya pindah ke Bandung, Indonesia. Isyana adalah putri bungsu dari pasangan Luana Marpanda, seorang guru musik, dan Sapta Dwikardana, Ph.D seorang dosen dan terapis (grafologis). Ia memiliki kakak perempuan bernama Rara Sekar Larasati, yang juga merupakan vokalis band bernama Banda Neira. Dibesarkan dalam keluarga pendidik, Isyana diperkenalkan ke dunia musik pada usia 4 tahun oleh ibunya. Isyana telah menguasai sejumlah instrumen. Termasuk piano, electone, flute, biola, dan saksofon.” > Pertanyaan: Siapa nama orang tua Isyana? #####\n",
      "##### Response #####\n",
      "Berdasarkan teks, nama orang tua Isyana adalah:\n",
      "\n",
      "*   **Luana Marpanda** (ibu), seorang guru musik.\n",
      "*   **Sapta Dwikardana, Ph.D** (ayah), seorang dosen dan terapis (grafologis).\n",
      "=== Token Usage: 212 (prompt), 56 (completions), 268 (total) ===\n",
      "\n",
      "##### Prompt: Sebutkan persamaan dan perbedaan antara gado-gado, ketoprak dan karedok #####\n",
      "##### Response #####\n",
      "Tentu, mari kita bahas persamaan dan perbedaan antara gado-gado, ketoprak, dan karedok:\n",
      "\n",
      "**Persamaan:**\n",
      "\n",
      "*   **Bahan Dasar:** Ketiga hidangan ini sama-sama menggunakan sayuran rebus sebagai bahan utama. Sayuran yang umum digunakan meliputi kacang panjang, bayam, tauge, kentang, tahu, tempe, dan telur rebus.\n",
      "*   **Saus Kacang:** Ketiganya disajikan dengan saus kacang sebagai bumbu utama. Meskipun resep saus kacang bisa sedikit berbeda, bahan dasarnya tetap kacang tanah yang dihaluskan.\n",
      "*   **Asal Daerah:** Ketiga hidangan ini berasal dari Jawa Barat, Indonesia.\n",
      "*   **Makanan Sehat:** Ketiganya relatif sehat karena kandungan sayurannya yang tinggi.\n",
      "\n",
      "**Perbedaan:**\n",
      "\n",
      "| Fitur          | Gado-Gado                               | Ketoprak                               | Karedok                               |\n",
      "| -------------- | --------------------------------------- | --------------------------------------- | --------------------------------------- |\n",
      "| **Bahan Tambahan** | Lontong/ketupat, kerupuk, emping        | Lontong/ketupat, kerupuk, bawang goreng | Tidak menggunakan lontong/ketupat/kerupuk |\n",
      "| **Saus Kacang**   | Lebih manis dan kental, sering ditambahkan sedikit asam jawa | Lebih gurih dan sedikit pedas, sering ditambahkan kecap manis | Lebih segar dan pedas, menggunakan cabai rawit yang diulek langsung ke saus kacang |\n",
      "| **Penyajian**    | Sayuran dan bahan lain dicampur dengan saus kacang | Sayuran dan bahan lain disiram dengan saus kacang | Sayuran disiram dengan saus kacang yang sudah dicampur dengan bumbu (termasuk cabai rawit) |\n",
      "| **Ciri Khas**    | Rasa manis dan campuran tekstur         | Rasa gurih dan aroma bawang goreng      | Rasa segar, pedas, dan aroma kacang yang kuat |\n",
      "| **Asal Daerah Spesifik** | Jakarta                               | Jakarta                               | Sunda (biasanya daerah Cianjur dan sekitarnya) |\n",
      "\n",
      "**Penjelasan Tambahan:**\n",
      "\n",
      "*   **Gado-gado** sering dianggap sebagai hidangan yang lebih \"kaya\" karena adanya lontong/ketupat dan kerupuk/emping yang menambah tekstur dan rasa.\n",
      "*   **Ketoprak** identik dengan aroma bawang goreng yang kuat dan rasa gurih yang dominan.\n",
      "*   **Karedok** adalah yang paling sederhana dan segar. Karedok tidak menggunakan lontong/ketupat/kerupuk, dan saus kacangnya cenderung lebih pedas dan segar karena penggunaan cabai rawit yang diulek.\n",
      "\n",
      "Semoga penjelasan ini membantu!\n",
      "=== Token Usage: 28 (prompt), 593 (completions), 621 (total) ===\n",
      "\n",
      "##### Prompt: Jelaskan budaya Indonesia menyapa orang yang lebih tua? #####\n",
      "##### Response #####\n",
      "Budaya Indonesia sangat menjunjung tinggi rasa hormat kepada orang yang lebih tua. Cara menyapa orang yang lebih tua di Indonesia sangat beragam, tergantung pada daerah, etnis, dan tingkat keakraban. Namun, ada beberapa prinsip umum dan contoh yang bisa dijelaskan:\n",
      "\n",
      "**Prinsip Umum:**\n",
      "\n",
      "*   **Menunduk:** Menundukkan kepala sedikit saat berbicara dengan orang yang lebih tua adalah tanda hormat yang umum di seluruh Indonesia.\n",
      "*   **Bahasa Tubuh yang Sopan:** Hindari kontak mata yang terlalu lama (terutama dengan orang yang sangat dihormati), jangan menyilangkan tangan di depan dada, dan jaga postur tubuh yang tegak namun tidak kaku.\n",
      "*   **Bahasa yang Halus:** Menggunakan bahasa yang sopan dan halus (bahasa *krama* di Jawa, misalnya) sangat penting. Hindari bahasa gaul atau bahasa yang terlalu santai.\n",
      "*   **Menawarkan Bantuan:** Menawarkan bantuan, terutama kepada orang tua atau yang kesulitan, adalah bentuk penghormatan yang sangat dihargai.\n",
      "*   **Menunggu Disapa Duluan:** Dalam beberapa situasi, terutama jika bertemu dengan orang yang sangat dihormati, lebih baik menunggu mereka menyapa terlebih dahulu.\n",
      "\n",
      "**Contoh Cara Menyapa Berdasarkan Daerah dan Tingkat Keakraban:**\n",
      "\n",
      "**1. Jawa & Bali:**\n",
      "\n",
      "*   **\"Sugeng ndalu/sore/wengi\" (Jawa) / \"Om Swastiastu\" (Bali):** Sapaan umum yang digunakan sepanjang hari.\n",
      "*   **\"Sampun dhahar?\" (Jawa):**  \"Sudah makan?\" - Pertanyaan ini menunjukkan perhatian dan kepedulian.\n",
      "*   **\"Kula nuwun sewu\" (Jawa):** \"Permisi\" - Digunakan saat ingin meminta sesuatu atau melewati orang yang lebih tua.\n",
      "*   **Menggunakan *krama inggil* (bahasa Jawa halus):**  Contoh: \"Panjenengan sehat?\" (Bagaimana kesehatan Anda?) bukan \"Kamu sehat?\".\n",
      "*   **Menyentuh tangan orang tua (Jawa):**  Setelah menyapa, biasanya menyentuh tangan orang tua dan membawanya ke dahi sebagai tanda hormat (tradisi ini semakin jarang dilakukan di perkotaan).\n",
      "*   **Di Bali:** Menyapa dengan *Om Swastiastu* dan seringkali disertai dengan memberikan persembahan kecil (seperti bunga atau daun sirih).\n",
      "\n",
      "**2. Sumatera (Melayu, Batak, Minang, dll.):**\n",
      "\n",
      "*   **\"Assalamualaikum\" (Melayu):** Sapaan umum di kalangan Muslim.\n",
      "*   **\"Horas\" (Batak):** Sapaan umum di kalangan suku Batak.\n",
      "*   **\"Salam alaikum\" (Minang):** Sapaan umum di kalangan suku Minang.\n",
      "*   **\"Adik/Kakak/Abang/Makcik/Pakcik\":**  Panggilan kekerabatan yang digunakan untuk menunjukkan rasa hormat, bahkan jika tidak ada hubungan darah.  Misalnya, memanggil wanita yang lebih tua dengan \"Makcik\" (bibi) atau pria yang lebih tua dengan \"Pakcik\" (paman).\n",
      "*   **Menawarkan tempat duduk:**  Sangat umum menawarkan tempat duduk kepada orang yang lebih tua.\n",
      "\n",
      "**3. Sulawesi (Bugis, Makassar, Toraja, dll.):**\n",
      "\n",
      "*   **\"Mappia' (Bugis/Makassar):** Sapaan umum.\n",
      "*   **\"Sikke' (Toraja):** Sapaan umum.\n",
      "*   **Menawarkan pinang dan sirih:**  Tradisi yang umum di beberapa daerah di Sulawesi sebagai tanda keramahan dan penghormatan.\n",
      "\n",
      "**4. Kalimantan (Dayak, Banjar, dll.):**\n",
      "\n",
      "*   **\"Hau\" (Dayak):** Sapaan umum.\n",
      "*   **\"Assalamualaikum\" (Banjar):** Sapaan umum di kalangan Muslim.\n",
      "*   **Menawarkan minuman atau makanan:**  Tanda keramahan dan penghormatan.\n",
      "\n",
      "**5. Nusa Tenggara (Sasak, Flores, Timor, dll.):**\n",
      "\n",
      "*   **\"Om Swastiastu\" (Bali - pengaruh budaya Bali):** Sapaan umum di beberapa pulau.\n",
      "*   **Sapaan lokal:** Setiap suku memiliki sapaan khasnya sendiri.\n",
      "*   **Menawarkan bantuan:**  Sangat dihargai.\n",
      "\n",
      "**6. Papua & Maluku:**\n",
      "\n",
      "*   **Sapaan lokal:** Setiap suku memiliki sapaan khasnya sendiri.\n",
      "*   **Menawarkan makanan atau minuman:**  Tanda keramahan dan penghormatan.\n",
      "\n",
      "**Penting untuk diingat:**\n",
      "\n",
      "*   **Konteks:**  Cara menyapa juga tergantung pada konteks situasinya. Menyapa tetangga di jalan akan berbeda dengan menyapa\n",
      "=== Token Usage: 21 (prompt), 1024 (completions), 1045 (total) ===\n",
      "\n",
      "##### Prompt: Jelaskan budaya pulang kampung ketika lebaran? #####\n",
      "##### Response #####\n",
      "Budaya pulang kampung saat Lebaran, atau yang dikenal dengan istilah \"mudik\" di Indonesia, adalah fenomena sosial dan budaya yang sangat kuat dan khas. Berikut penjelasan lengkapnya:\n",
      "\n",
      "**1. Definisi dan Latar Belakang:**\n",
      "\n",
      "*   **Mudik:** Secara harfiah berarti \"pulang\" atau \"kembali\". Dalam konteks Lebaran, mudik merujuk pada perjalanan pulang ke kampung halaman, biasanya ke desa atau kota asal keluarga, yang dilakukan oleh perantau atau orang yang bekerja dan tinggal di kota-kota besar.\n",
      "*   **Latar Belakang:** Tradisi mudik berakar kuat pada nilai-nilai kekeluargaan, gotong royong, dan tradisi Islam. Lebaran adalah momen penting untuk mempererat tali silaturahmi dengan keluarga besar, meminta maaf atas kesalahan, dan berbagi kebahagiaan. Bagi banyak orang, kampung halaman adalah tempat di mana mereka merasa memiliki identitas dan akar budaya yang kuat.\n",
      "\n",
      "**2. Prosesi dan Tradisi Mudik:**\n",
      "\n",
      "*   **Persiapan:** Persiapan mudik biasanya dimulai jauh-jauh hari sebelum Lebaran. Ini meliputi:\n",
      "    *   **Pembelian Tiket:** Membeli tiket transportasi (kereta api, bus, pesawat, kapal laut) yang seringkali sangat sulit karena tingginya permintaan.\n",
      "    *   **Persiapan Barang Bawaan:** Mempersiapkan pakaian lebaran baru, oleh-oleh untuk keluarga, dan uang tunai (karena fasilitas perbankan di kampung mungkin terbatas).\n",
      "    *   **Membersihkan Rumah:** Membersihkan dan merapikan rumah di kampung halaman, seringkali dibantu oleh tetangga atau saudara yang sudah lebih dulu tiba.\n",
      "    *   **Zakat Fitrah:** Membayar zakat fitrah sebagai kewajiban agama sebelum Hari Raya.\n",
      "*   **Perjalanan:** Perjalanan mudik seringkali menjadi tantangan tersendiri karena:\n",
      "    *   **Kepadatan Lalu Lintas:** Jalan raya dan terminal transportasi sangat padat, menyebabkan kemacetan parah.\n",
      "    *   **Kelelahan:** Perjalanan bisa memakan waktu berjam-jam atau bahkan berhari-hari, terutama bagi mereka yang menggunakan transportasi darat.\n",
      "    *   **Biaya:** Biaya transportasi dan akomodasi selama mudik bisa cukup besar.\n",
      "*   **Tiba di Kampung Halaman:** Kedatangan di kampung halaman disambut dengan hangat oleh keluarga dan tetangga.\n",
      "*   **Tradisi Lebaran:** Setelah tiba, berbagai tradisi Lebaran dijalankan:\n",
      "    *   **Shalat Idul Fitri:** Melaksanakan shalat Idul Fitri berjamaah di masjid atau lapangan.\n",
      "    *   **Silaturahmi:** Mengunjungi rumah keluarga besar, tetangga, dan teman-teman untuk saling bermaaf-maafan.\n",
      "    *   **Open House:** Menerima tamu di rumah untuk berbagi hidangan Lebaran dan bersilaturahmi.\n",
      "    *   **Makan-makan:** Menikmati hidangan khas Lebaran seperti ketupat, opor ayam, rendang, dan kue-kue tradisional.\n",
      "    *   **Tradisi Lokal:** Setiap daerah memiliki tradisi Lebaran yang unik, seperti pertunjukan seni, lomba, atau upacara adat.\n",
      "\n",
      "**3. Makna Budaya Mudik:**\n",
      "\n",
      "*   **Mempererat Tali Silaturahmi:** Mudik adalah wujud nyata dari pentingnya menjaga hubungan baik dengan keluarga dan kerabat.\n",
      "*   **Menghidupkan Ekonomi Lokal:** Arus mudik memberikan dampak positif bagi perekonomian di kampung halaman, terutama bagi pedagang kecil dan penyedia jasa.\n",
      "*   **Melestarikan Budaya:** Mudik menjadi sarana untuk melestarikan tradisi dan budaya lokal, terutama bagi generasi muda yang tumbuh di kota.\n",
      "*   **Refleksi Diri:** Momen Lebaran dan mudik seringkali menjadi waktu untuk merenungkan diri, memperbaiki diri, dan memperbarui komitmen untuk menjadi pribadi yang lebih baik.\n",
      "*   **Identitas dan Akar Budaya:** Mudik mengingatkan perantau akan identitas dan akar budaya mereka, serta memberikan rasa memiliki dan kebanggaan.\n",
      "\n",
      "**4. Perubahan dan Tantangan Mudik Modern:**\n",
      "\n",
      "*   **Peningkatan Mobilitas:** Semakin banyak orang merantau ke kota-kota besar, sehingga arus mudik semakin meningkat setiap tahun.\n",
      "*   **Diversifikasi Transportasi:** Selain transportasi tradisional, kini banyak orang memilih menggunakan sepeda motor, mobil pribadi, atau bahkan ojek online untuk mudik.\n",
      "*   **Mudik Virtual:** Di era digital, mudik virtual (melalui video call atau media sosial) menjadi alternatif bagi mereka yang tidak bisa pulang kampung secara fisik.\n",
      "*   **Tantangan Infrastruktur:** Keterbatasan infrastruktur transportasi dan akomodasi di beberapa daerah masih menjadi tantangan utama dalam pelaksanaan mudik.\n",
      "*\n",
      "=== Token Usage: 19 (prompt), 1024 (completions), 1043 (total) ===\n",
      "\n",
      "##### Prompt: Sebutkan berbagai jenis kopi dan karakteristik rasanya yang berasal dari Indonesia #####\n",
      "##### Response #####\n",
      "Indonesia adalah surga bagi pecinta kopi, dengan berbagai jenis kopi yang memiliki karakteristik rasa unik. Berikut adalah beberapa jenis kopi Indonesia yang populer beserta karakteristik rasanya:\n",
      "\n",
      "**1. Kopi Gayo (Aceh)**\n",
      "\n",
      "*   **Asal:** Dataran Tinggi Gayo, Aceh Tengah dan Bener Meriah.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Penuh dan tebal (full-bodied).\n",
      "    *   **Aroma:** Floral, herbal, dan sedikit rempah.\n",
      "    *   **Rasa:** Kompleks, dengan sentuhan cokelat, karamel, dan sedikit rasa buah (cherry atau berry).  Seringkali memiliki rasa earthy (tanah) yang khas.\n",
      "    *   **Acidity:** Sedang hingga tinggi, memberikan kesegaran.\n",
      "*   **Grade:** Umumnya Grade 1 (kualitas terbaik).\n",
      "\n",
      "**2. Kopi Toraja (Sulawesi Selatan)**\n",
      "\n",
      "*   **Asal:** Dataran Tinggi Toraja, Sulawesi Selatan.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Penuh dan lembut.\n",
      "    *   **Aroma:** Fruity (buah-buahan), floral, dan sedikit nutty (kacang).\n",
      "    *   **Rasa:**  Cokelat gelap, karamel, dan sedikit rasa rempah.  Toraja dikenal dengan rasa manis alami dan sedikit rasa asam yang menyegarkan.\n",
      "    *   **Acidity:** Rendah hingga sedang.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**3. Kopi Mandailing (Sumatera Utara)**\n",
      "\n",
      "*   **Asal:** Daerah Mandailing, Sumatera Utara.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sangat penuh dan tebal (heavy-bodied).\n",
      "    *   **Aroma:** Earthy, herbal, dan sedikit rempah.\n",
      "    *   **Rasa:** Cokelat gelap, rempah-rempah, dan sedikit rasa buah (plum atau berry).  Mandailing seringkali memiliki rasa yang kuat dan kompleks.\n",
      "    *   **Acidity:** Rendah.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**4. Kopi Lintong (Sumatera Utara)**\n",
      "\n",
      "*   **Asal:** Daerah Lintong, Sumatera Utara.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Penuh dan lembut.\n",
      "    *   **Aroma:** Fruity, floral, dan sedikit herbal.\n",
      "    *   **Rasa:** Cokelat, karamel, dan sedikit rasa buah (jeruk atau apel).  Lintong dikenal dengan rasa yang seimbang dan lembut.\n",
      "    *   **Acidity:** Sedang.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**5. Kopi Flores (Nusa Tenggara Timur)**\n",
      "\n",
      "*   **Asal:** Pulau Flores, Nusa Tenggara Timur.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sedang hingga penuh.\n",
      "    *   **Aroma:** Floral, fruity, dan sedikit nutty.\n",
      "    *   **Rasa:** Cokelat, karamel, dan sedikit rasa buah (plum atau berry).  Flores seringkali memiliki rasa yang unik dan kompleks.\n",
      "    *   **Acidity:** Sedang hingga tinggi.\n",
      "*   **Grade:** Umumnya Grade 1.\n",
      "\n",
      "**6. Kopi Java (Jawa)**\n",
      "\n",
      "*   **Asal:** Pulau Jawa (terutama daerah Ijen, Preanger, dan Lampung).\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sedang.\n",
      "    *   **Aroma:** Cokelat, rempah-rempah, dan sedikit floral.\n",
      "    *   **Rasa:** Cokelat, karamel, dan sedikit rasa rempah.  Java dikenal dengan rasa yang lembut dan seimbang.\n",
      "    *   **Acidity:** Rendah.\n",
      "*   **Grade:** Bervariasi, tergantung daerah dan proses.\n",
      "\n",
      "**7. Kopi Bali (Bali)**\n",
      "\n",
      "*   **Asal:** Pulau Bali.\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sedang hingga penuh.\n",
      "    *   **Aroma:** Floral, fruity, dan sedikit cokelat.\n",
      "    *   **Rasa:** Cokelat, karamel, dan sedikit rasa buah (jeruk atau apel).  Bali dikenal dengan rasa yang lembut dan seimbang.\n",
      "    *   **Acidity:** Sedang.\n",
      "*   **Grade:** Bervariasi, tergantung daerah dan proses.\n",
      "\n",
      "**8. Kopi Luwak (Seluruh Indonesia)**\n",
      "\n",
      "*   **Asal:** Seluruh Indonesia (terutama Sumatera, Jawa, dan Bali).\n",
      "*   **Karakteristik Rasa:**\n",
      "    *   **Body:** Sedang hingga\n",
      "=== Token Usage: 23 (prompt), 1024 (completions), 1047 (total) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in SAMPLE_PROMPTS:\n",
    "    print(f\"##### Prompt: {prompt} #####\")\n",
    "    print(f\"##### Response #####\")\n",
    "    _ = invoke_sealion(prompt, max_tokens=1024, temperature=0.1, top_p=0.9)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "## Multimodality\n",
    "\n",
    "SEA LION v4 models (based on Gemma 3) are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731037a-2649-4b40-b530-135d36ae706c",
   "metadata": {},
   "source": [
    "#### single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36b8a8a0-fde2-4286-b1cf-30bf6b069800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(url=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23894c86-9b8e-463c-84d1-dd6eef6f8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sealion_multimodal(prompt: str, image_url: str, system_prompt=\"You are a helpful assistant.\", print_response=True, **kwargs):\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": system_prompt\n",
    "                  }\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\", \n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    for k in kwargs:\n",
    "        if k in ['max_tokens', 'temperature', 'top_p']:\n",
    "            payload[k] = kwargs[k]\n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    if print_response:\n",
    "        # Print usage statistics\n",
    "        usage = response['usage']\n",
    "        print(response['choices'][0]['message']['content'].strip())\n",
    "        print(f\"=== Token Usage: {usage['prompt_tokens']} (prompt), {usage['completion_tokens']} (completions), {usage['total_tokens']} (total) ===\")\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74679f16-f21f-4924-b2bf-ebe2ec6aab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, the animal on the candy appears to be a **turtle**. \n",
      "\n",
      "You can see the shape of a turtle shell and head on each of the candies.\n",
      "=== Token Usage: 282 (prompt), 37 (completions), 319 (total) ===\n"
     ]
    }
   ],
   "source": [
    "_ = invoke_sealion_multimodal(\n",
    "    prompt = \"What animal is on the candy?\",\n",
    "    image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f842f1e-1f59-496b-85ee-3b0a3bd85951",
   "metadata": {},
   "source": [
    "### Streaming responses\n",
    "You can also direclty stream response from your endpoint. To achieve this, we will use the invoke_endpoint_with_response_stream API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362629-77fe-4b57-b1ab-97259defc72b",
   "metadata": {},
   "source": [
    "You can **interleave images with text**. To do so, just cut off the input text where you want to insert an image, and insert it with an image block like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54a9c62e-fb42-409a-b1f5-0c4084662fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "# IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3018.JPG\", height=300, width= 300)\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf3fb62f-16c3-4eef-9f6e-9f82ba78cc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "# IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\", height=300, width= 300)\n",
    "IPyImage(url=\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\", height=300, width= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8d4f5f-fc32-468f-b4d0-3551c9327ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"I have this \"},  # \"I'm already using this supplement \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/candy.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"and I want to use this as well \"},\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/fruit_knife.png\"}, # \"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/IMG_3015.jpg\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \" what are cautions?\"},\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 1500,\n",
    "  \"temperature\": 0.6,\n",
    "  \"top_p\": 0.9,\n",
    "  \"stream\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27e3b708-cf53-402d-8dff-d3b465775fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: gemma-sea-lion-v4-27b-it-250901-0659\n",
      "Response: Okay I see the images. You have a hand holding colorful, small candies (likely Jelly Belly brand, judging by the shape and design) and a picture of various fruits, a knife, and a cutting board. \n",
      "\n",
      "Here are some cautions, considering you might be thinking of combining these images or concepts:\n",
      "\n",
      "**Regarding the Candies & Fruit:**\n",
      "\n",
      "*   **Sugar Intake:** Combining images of candy with fruit could imply a focus on sugary foods. Be mindful of promoting a balanced diet. Too much sugar isn't healthy.\n",
      "*   **Choking Hazard:** The candies are small. If you're using this in a context where young children might see it, it's important to note they could be a choking hazard.\n",
      "*   **Misleading Association:** Don't present the candies *as if* they are a healthy alternative to fruit. They are very different nutritionally.\n",
      "\n",
      "**Regarding the Knife & Fruit (and potentially the candies):**\n",
      "\n",
      "*   **Sharp Objects:** The knife is a potential hazard. If you're using this image in a context where children might see it, be very careful about how it's presented. Avoid showing anyone, especially children, handling the knife.\n",
      "*   **Safe Food Handling:** If you're depicting food preparation, ensure it aligns with safe food handling practices (e.g., clean surfaces, proper knife techniques).\n",
      "*   **Potential for Injury:** If you're thinking of visually combining the candies and the knife (e.g., \"cutting\" the candies), this could be interpreted as promoting unsafe behavior.\n",
      "\n",
      "**General Cautions:**\n",
      "\n",
      "*   **Context is Key:** The specific cautions depend *entirely* on how you intend to use these images. What is the purpose of combining them? Are you making a collage, a meme, a presentation, or something else?\n",
      "*   **Target Audience:** Who will be viewing this? (Children, adults, etc.) This greatly impacts the level of caution needed.\n",
      "\n",
      "\n",
      "\n",
      "**To help me give you more specific advice, could you tell me:**\n",
      "\n",
      "*   **What are you planning to *do* with these images?** (e.g., create a graphic, write a story, etc.)\n",
      "*   **Who is your intended audience?**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics:\n",
      "Time to First Token (TTFT): 0.01 seconds\n",
      "Total Tokens Generated: 470\n",
      "Total Latency: 18.55 seconds\n",
      "Tokens per second: 25.33\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "print(f'Using endpoint: {endpoint_name}')\n",
    "\n",
    "# Invoke the model\n",
    "response_stream = sagemaker_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = \"application/json\",\n",
    "    Body = json.dumps(body)\n",
    ")\n",
    "\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_count += 1\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                                    # Record time to first token\n",
    "                    if not first_token_received:\n",
    "                        ttft = time.time() - start_time\n",
    "                        first_token_received = True\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "# Print metrics after completion\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds\" if ttft else \"TTFT: N/A\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "if token_count > 0 and total_latency > 0:\n",
    "    print(f\"Tokens per second: {token_count/total_latency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9783d3-a8e4-4c86-81f8-054e2175ce5a",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd76446-9f4b-4bb6-b175-4a47e431b0f8",
   "metadata": {},
   "source": [
    "Note that failure to delete the endpoint will incur costs.\n",
    "\n",
    "The ml.g5.12xlarge and ml.g6e.48xlarge instances costs (for running 24/7 SageMaker Realtime Inference in us-east-1) are:\n",
    "\n",
    "Instance Type     | Cost/hr   | Cost/day  | Cost/month\n",
    "------------------|-----------|-----------|-------------\n",
    "`ml.g5.12xlarge`  | USD  7.09 |USD 170.16 |USD  5,175.70\n",
    "`ml.g6e.48xlarge` | USD 37.664|USD 903.936|USD 27,494.72\n",
    "\n",
    "It is highly recommended that you delete endpoints that are no longer in use, or that you choose an option such as [SageMaker Scale down to zero feature](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-zero-instances.html) as mentioned in this blog post: [Unlock cost savings with the new scale down to zero feature in SageMaker Inference](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e2269-2f95-49e3-803f-5110d2023b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to generate html version of the Jupyter notebook\n",
    "!jupyter nbconvert Gemma-SEA-LION-v4-27B-IT.ipynb --to html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
