# Running SEA-LION Models for Inference

SEA-LION models are freely available for [download](/models/download_models.md), and can be run for inference in various ways. The following guides provide technical how-tos for setting up SEA-LION inference using different approaches:

1. [Using our provided SEA-LION API](./api.md)
2. Running SEA-LION on a local machine
3. Deploying SEA-LION on the cloud
    - [Create SEA-LION endpoint on Google Vertex AI](./vertex_ai.md)
    - [Importing and Using Llama-SEA-LION models in a Serverless On-Demand Environment with Amazon Bedrock](./amazon_bedrock.md)
    - [OpenAI-compatible APIs with Llama-SEA-LION models and Bedrock Access Gateway](./bedrock_access_gateway.md)
    - [Deploying Gemma-SEA-LION models using AWS Sagemaker AI](./Gemma-SEA-LION-v4-27B-Instruct.ipynb)
    - [Deploying SEA-LION using vLLM on Linux server](./vllm_linux.md)

